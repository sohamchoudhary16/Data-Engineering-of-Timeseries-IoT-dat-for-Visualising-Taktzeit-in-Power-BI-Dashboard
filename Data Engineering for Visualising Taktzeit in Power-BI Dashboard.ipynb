{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c06964d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import datetime\n",
    "from functools import reduce\n",
    "import functools \n",
    "import pytz\n",
    "import argparse\n",
    "import boto3\n",
    "import sagemaker_pyspark\n",
    "import botocore.session\n",
    "\n",
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "conf = (SparkConf()\n",
    "        .set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars())))\n",
    "\n",
    "\n",
    "spark = (SparkSession.builder\\\n",
    "         .config(conf=conf)\\\n",
    "         .config('fs.s3a.access.key', credentials.access_key)\\\n",
    "         .config('fs.s3a.secret.key', credentials.secret_key)\\\n",
    "         .appName('HRI')\\\n",
    "         .config(\"spark.executor.memory\", \"70g\")\\\n",
    "         .config(\"spark.driver.memory\", \"50g\")\\\n",
    "         .config(\"spark.memory.offHeap.enabled\",True)\\\n",
    "         .config(\"spark.memory.offHeap.size\",\"16g\")\\\n",
    "         .getOrCreate())\n",
    "\n",
    "df_input = spark.read.csv(\"contour-export-22-02-2022.csv\", inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31144e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prozent_taktzeit(df):    \n",
    "    df = df.groupBy('dashboard_name', 'granularity', 'date', 'coalesce_process_step_code', 'time_start', 'time_end', 'part_id', 'part_type')\\\n",
    "           .agg((((F.count(when(F.col('time_diff') < 60, True)))-(F.count(when(F.col('time_diff') == 0, True))))/((F.count(when(F.col('time_diff') > 1, True))))).alias(\"prozent_taktzeit\"))\n",
    "    return df\n",
    "\n",
    "def median_taktzeit(df):\n",
    "    df = df.withColumn(\"median\",when(F.col('time_diff') != 0, (F.col('time_diff'))))\n",
    "    magic_percentile = F.expr(\"percentile_approx(median, 0.5)\")\n",
    "    df = df.groupBy('dashboard_name', 'granularity', 'date', 'coalesce_process_step_code', 'time_start', 'time_end', 'part_id', 'part_type').agg(magic_percentile.alias('median_taktzeit')).drop(\"median\")\n",
    "    return df\n",
    "\n",
    "def half_hour_cycle_time(df):\n",
    "    df = df.withColumn(\"date_convert\", F.to_date(\"UTC\", \"yyyy-MM-dd\"))\n",
    "    df = df.withColumn(\"date\", F.to_timestamp(\"date_convert\").cast('string')).drop('date_convert')\n",
    "    df = df.withColumn('hour', hour(df.time))\n",
    "    df = df.withColumn('minute', minute(df.time))\n",
    "    df = df.orderBy(F.col('UTC').desc())\n",
    "    df = df.withColumn('granularity', lit('30min'))\n",
    "    # add start time half hour \n",
    "    df = df.withColumn('time_start',\n",
    "                      when((F.col('minute')>=0) & (F.col('minute')<30), (F.date_format((F.unix_timestamp('time', 'HH:mm')).cast('timestamp'), 'HH:00:00')))\\\n",
    "                     .when((F.col('minute')>=30) & (F.col('minute')<0), (F.date_format((F.unix_timestamp('time', 'HH:mm')).cast('timestamp'), 'HH:30:00')))\\\n",
    "                     .otherwise(F.date_format((F.unix_timestamp('time', 'HH:mm')).cast('timestamp'), 'HH:30:00')))\n",
    "    # add time end half hour\n",
    "    df = df.withColumn('time_30_min', F.col('time_start') + F.expr('INTERVAL 30 MINUTES'))\n",
    "    df = df.withColumn(\"time_end\", date_format(\"time_30_min\", 'HH:mm:ss')).drop(\"time_30_min\", \"hour\", \"minute\")\n",
    "    return df\n",
    "\n",
    "def daily_cycle_time(df):\n",
    "    df = df.withColumn(\"date_new\", F.to_date(\"UTC\", \"yyyy-MM-dd\"))\n",
    "    df = df.sort(df.measurement_datetime.desc())\n",
    "    df = df.withColumn(\"day_sub\", date_add(date_format('date_new', 'yyyy-MM-dd'),-1))\n",
    "    df = df.withColumn('date_convert', \n",
    "                      when((F.col(\"time\")>=\"00:00:00\") & (F.col(\"time\")<\"04:55:00\"),F.col(\"day_sub\")) \\\n",
    "                     .when((F.col(\"time\")>=\"04:55:00\") & (F.col(\"time\")<\"00:00:00\"), F.col(\"date_new\")) \\\n",
    "                     .otherwise(F.col(\"date_new\")))\n",
    "    df = df.withColumn(\"date\", F.to_timestamp(\"date_convert\").cast('string')).drop('date_convert')\n",
    "    # add time end half hour\n",
    "    df = df.withColumn('granularity', lit('daily_4_55hmm'))\n",
    "    df = df.withColumn('time_start_to_timestamp', F.to_timestamp(lit('04:55:00'), 'HH:mm:ss'))\n",
    "    df = df.withColumn(\"time_start\", date_format(\"time_start_to_timestamp\", 'HH:mm:ss'))\n",
    "    df = df.withColumn('time_end_to_timestamp', F.to_timestamp(lit('04:55:00'), 'HH:mm:ss'))\n",
    "    df = df.withColumn(\"time_end\", date_format(\"time_end_to_timestamp\", 'HH:mm:ss')).drop(\"date_new\", \"day_sub\", \"time_start_to_timestamp\", \"time_end_to_timestamp\")\n",
    "    return df\n",
    "\n",
    "def part_cycle_time(df):\n",
    "    # add the granularity part for dynamic takt calc\n",
    "    df = df.withColumn(\"date\", F.to_date(\"UTC\", \"yyyy-MM-dd\"))\n",
    "    df = df.withColumn('granularity', lit('AFO'))\n",
    "    w_part = Window.partitionBy('coalesce_process_step_code').orderBy(F.col('UTC').desc())\n",
    "    df = df.withColumn('time_start', date_format(F.lead(\"UTC\",10).over(w_part), 'HH:mm:ss'))\n",
    "    df = df.withColumn('time_end', date_format(F.col(\"UTC\"), 'HH:mm:ss'))\n",
    "    return df\n",
    "\n",
    "def unionAll(dfs):\n",
    "    return functools.reduce(lambda df_daily_455,df_half_hour: df_daily_455.union(df_half_hour.select(df_daily_455.columns)), dfs) \n",
    "\n",
    "def join_col(df1, df2, condition, how='inner'):\n",
    "    df = df1.join(df2, condition, how=how)\n",
    "    repeated_columns = [c for c in df1.columns if c in df2.columns]\n",
    "    for col in repeated_columns:\n",
    "        df = df.drop(df2[col])\n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
